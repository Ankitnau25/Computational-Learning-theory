Subfields of statistical learning theory:
The goal is to study learning algorithms that are algorithms who receive some data, and provide and give us result, and estimation function f.
Our goal is to prove performance guarantees on these learning algorithms, the same is done in statistics, but there are people consider very specific distributions on both the input X, and the output Y. For example, they assume the Gaussian distributed a mixture of Gaussian exponential families. The distributions they consider they belong to a family that can be parameterized by a few parameters. In statistical learning theory, we make no or just very general assumptions on the distributions. These assumptions never allow us to parameterize the family of distributions and satisfy them. 
These subfields are defined by what is the set Y, the set of outputs or labels or targets, and what data is given, and how it is represented. We will start with problems where all data is given at the beginning, and we have to produce an estimation function, and then our job is done. It's called batch learning.
Subfileds can be on basis of all data given Y fixed classification cluster Y unfixed regression ,Diminsionality reduction
Subfiled can be supervised online batch , active reinforcment

Overfit to get rich:-
If tree is small then its ike small company useful else its not good. Neural net are exception

3 levels
Reproductive learning: problamatic unseen data
Rule based : threshold on numbers doctors based
Creative learning : rotation picture
